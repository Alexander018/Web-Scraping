#!/usr/bin/env python3
import email
from lib2to3.pgen2 import driver
from unittest import result
import requests
from bs4 import BeautifulSoup
from time import *
from random import randint
from selenium import webdriver
import time
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys  
import pandas as pd


def get_section(abb, excel):
    URL = 'https://app.testudo.umd.edu/soc/202208/' + abb
  
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, "html.parser")
    
    
    results = soup.find(id ="courses-page")

    sections = results.find_all("div", class_="course")

    for section in sections:
        dict = {}
        cls = section.find("div", class_="course-id")
        dict["Section"] = cls.text
        print(cls.text)
        b = cls.text
        excel.append(dict)
        get_data(abb, b, excel)
        #sleep(18)
        # sec_id = section.find("span", class_="section-id")
        # sec_instructor = section.find("span", class_="section-instructor")
        # total_seats = section.find("span", class_="total-seats-count")
        # open_seats = section.find("span", class_="open-seats-count")
        # print(cls.text)
        # #the below code wont work do to the expand toogle not being clicked on the website when loaded. Use selinum to fix the issue 
        # print(sec_id.txt)# fails here
        # print(sec_instructor.txt)
        # print(total_seats.txt)
        # print(open_seats.txt)
        #print()


def get_data(a, b, excel): 
    URL = 'https://app.testudo.umd.edu/soc/202308/' + a + '/' + b
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, "html.parser")
    results = soup.find(id ="courses-page")
    #print(URL)
    
    sections = results.find_all("div", class_="section delivery-f2f")
    #print(sections)
    for section in sections:
        #cls = section.find("div", class_="course-id")
            dict = {}
            #count = 1
            sec_id = section.find("span", class_="section-id")
            sec_instructor = section.find("span", class_="section-instructor")
            total_seats = section.find("span", class_="total-seats-count")
            open_seats = section.find("span", class_="open-seats-count")
            # print(count)
            # count += 1
            print("Section: " + sec_id.get_text().strip())
            print("instructor: " + sec_instructor.get_text())
            # search(sec_instructor.get_text())
           
            dict["Section"] =  sec_id.get_text().strip()
            dict["Instructor"] = sec_instructor.get_text()
            dict["email"] = search(sec_instructor.get_text())
            dict["Total seats"] = total_seats.get_text()
            dict["Open Seats"] = open_seats.get_text()
            excel.append(dict)
            print("total seats: " + total_seats.get_text())
            print("Open seats: " + open_seats.get_text())
            
            print()
            #sleep(18)

def search(name):
    URL = 'https://identity.umd.edu/search'
    options = webdriver.ChromeOptions()
    options.add_argument('--ignore-certificate-errors')
    options.add_argument('--incognito')
    options.add_argument('--headless')
    #driver = webdriver.Chrome("/usr/bin/chromedriver", chrome_options=options)
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(URL)
    #print(driver.title)
    #search_bar = driver.find_element_by_name("basicSearchInput")
    search_bar = driver.find_element("name", "basicSearchInput")
    search_bar.clear()
    search_bar.send_keys(name)
    search_bar.send_keys(Keys.RETURN)

    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'lxml')
    #print(soup)
    results = soup.find("div", class_="email")
    print("email: " + results.get_text())
    email = results.get_text()
    # block = results.find_all("DetailBLock")
    # email = block.find("div", class_="email")
    # print(email.get_text())
    # sleep(7)
    driver.close()
    #sleep(15)
    return email        
       
def export_data(excel):
    df  = pd.DataFrame(excel)
    df.to_excel("libData.xlsx")
    df.to_csv("libData.csv")

def main():
    excel = []
    URL = "https://app.testudo.umd.edu/soc/"
    page = requests.get(URL)#this gets all the data from web

    soup = BeautifulSoup(page.content,"html.parser")#this parses the webpage

    results = soup.find(id ="course-prefixes-page")#this finds the part of the webpage I want 
    #print(results.prettify())

    class_codes = results.find_all("div", class_="course-prefix row")#this finds all the tags we want from the webpage

    for class_code in class_codes:#this loops through every tag and gets its
        class_abb = class_code.find("span", class_="prefix-abbrev push_one two columns")
        #class_name = class_code.find("span", class_="prefix-name nine columns")
        #print(class_abb.text)
        abb = "" + class_abb.text
        #print(class_name.text)
        #print()
        get_section(abb, excel)
        #x = randint(10,28)#this generates a random number between 2 and 5 sec
        #sleep(x)#this is to pretend to be human it will resume scrapping after x sec

        #print(class_code, end="\n"*2)
    #print(page.text)
    #export_data(excel)
    print("done")


if __name__ == '__main__':
  main()  


